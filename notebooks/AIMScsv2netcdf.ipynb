{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4891e3c1",
   "metadata": {},
   "source": [
    "## Let's use the netcdf tools and known method to create the netcdf format I want.\n",
    "## seems that xarray is peculiar to gridded data and treats time very specifically and I can't get past it needing to add a dimension to time and use it as a coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b02cb0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "# include the IMOS python toolbox to assist with attributes\n",
    "sys.path.insert(1, '/oa-decadal-climate/work/observations/oceanobs_data/UOT/programs/data-services/lib/python/')\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset, date2num\n",
    "from generate_netcdf_att import generate_netcdf_att, get_imos_parameter_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9398bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LATITUDE': 'LATITUDE', 'LONGITUDE': 'LONGITUDE', 'TIME': 'SAMPLE DATE', 'BOT_DEPTH': 'TO DEPTH', 'TEMP': 'Temp', 'PRES_REL': 'Pres', 'PSAL': 'Salinity'}\n",
      "{'STATION NAME': 'cruise', 'DISCLAIMER': 'disclaimer', 'ATTRIBUTION': 'attribution', 'COPYRIGHT': 'license'}\n"
     ]
    }
   ],
   "source": [
    "# set up dictionaries to map the cast dimensioned names to the aims names\n",
    "# Have ignored all other parameters for now except for PSAL, TEMP, PRES_REL, DEPTH. The other ones have mixed units and \n",
    "# haven't the time to parse the units appropriately.\n",
    "imosnames = ['LATITUDE', 'LONGITUDE', 'TIME', 'BOT_DEPTH','TEMP','PRES_REL','PSAL']\n",
    "aimsnames = ['LATITUDE','LONGITUDE','SAMPLE DATE', 'TO DEPTH','Temp','Pres','Salinity']\n",
    "vardict = dict(zip(aimsnames,imosnames))\n",
    "vardict2 = dict(zip(imosnames,aimsnames))\n",
    "print(vardict2)\n",
    "imosglobnames = ['cruise', 'disclaimer', 'attribution', 'license']\n",
    "aimsglobnames = ['STATION NAME', 'DISCLAIMER', 'ATTRIBUTION', 'COPYRIGHT']\n",
    "globdict = dict(zip(aimsglobnames,imosglobnames))\n",
    "print(globdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47c2e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's read in the csv CTD data from AIMS\n",
    "AIMS_data_path ='/oa-decadal-climate/work/observations/CARSv2_ancillary/AIMS/'\n",
    "dir_contents = os.listdir(AIMS_data_path)\n",
    "\n",
    "filelist = Path(AIMS_data_path).rglob('*.csv')# read the csv file\n",
    "for filn in filelist:\n",
    "    \n",
    "    # set up the output file name:\n",
    "    nn = os.path.splitext(os.path.basename(filn))\n",
    "    outfile = os.path.join(os.path.dirname(filn), 'NC', nn[0]) + '.nc'\n",
    "    \n",
    "    # read the data:\n",
    "    df = pd.read_csv(filn, skiprows=15)\n",
    "\n",
    "    # header information\n",
    "    dfhead = pd.read_csv(filn, skiprows=range(15, 9999))\n",
    "    # if no time field, continue - there are a few without any date/time information\n",
    "    if not dfhead[names[0]].str.contains(vardict2['TIME']).any():\n",
    "        continue\n",
    "    \n",
    "    # the parameter names vary from file to file\n",
    "    dfgroup = df.groupby('PARAMETER')\n",
    "    \n",
    "    # read a file with the global attributes included and the nc configuration file\n",
    "    conf_file_generic = '/tube1/cow074/Documents/cars-v2/notebooks/generate_nc_file_att'\n",
    "    # get the data from the header\n",
    "    dfhead = pd.read_csv(filn, skiprows=range(15, 9999))\n",
    "    names = dfhead.columns\n",
    "\n",
    "    # get the coordinate/depth dimension\n",
    "    depth = df.loc[df['PARAMETER'].str.contains('Depth'), 'VALUE']\n",
    "    # get the other coordinates\n",
    "    lat = dfhead.loc[dfhead[names[0]].str.contains(vardict2['LATITUDE']), names[1]].item()\n",
    "    lon = dfhead.loc[dfhead[names[0]].str.contains(vardict2['LONGITUDE']), names[1]].item()\n",
    "    time = dt.datetime.strptime(dfhead.loc[dfhead[names[0]].str.contains(vardict2['TIME']), names[1]].item(), '%d-%m-%Y')\n",
    "\n",
    "    # create a netcdf object and write depth,time,lat,long to it:\n",
    "    with Dataset(outfile, 'w', format='NETCDF4') as output_netcdf_obj:\n",
    "        # first create our DEPTH dimension and variable\n",
    "        output_netcdf_obj.createDimension(\"DEPTH\", depth.size)\n",
    "        output_netcdf_obj.createVariable(\"DEPTH\", \"f\", \"DEPTH\")\n",
    "        output_netcdf_obj['DEPTH'][:] = depth\n",
    "        # and lat/lon/time vars which come from the header in the csv file:\n",
    "        output_netcdf_obj.createVariable('TIME','d', fill_value=get_imos_parameter_info('TIME', '_FillValue'))\n",
    "\n",
    "        output_netcdf_obj.createVariable(\"LATITUDE\", \"f\", fill_value=get_imos_parameter_info('LATITUDE', '_FillValue'))\n",
    "        output_netcdf_obj['LATITUDE'][:] = lat\n",
    "        output_netcdf_obj.createVariable(\"LONGITUDE\", \"f\", fill_value=get_imos_parameter_info('LONGITUDE', '_FillValue'))\n",
    "        output_netcdf_obj['LONGITUDE'][:] = lon  \n",
    "\n",
    "        # now all the other variables\n",
    "        for group in df['PARAMETER'].unique():\n",
    "            data = np.ma.masked_invalid(dfgroup.get_group(group)['VALUE'])\n",
    "            flag = np.ma.masked_invalid(dfgroup.get_group(group)['QAQC_FLAG'])\n",
    "            for value in vardict:\n",
    "                if value in group:\n",
    "                    name = vardict[value]\n",
    "                    #create the variable & QC variable:\n",
    "                    output_netcdf_obj.createVariable(name, \"f\", [\"DEPTH\"], \n",
    "                                fill_value=get_imos_parameter_info(name, '_FillValue'))\n",
    "                    output_netcdf_obj.createVariable(name + '_quality_control', \"b\", [\"DEPTH\"], \n",
    "                                fill_value=99)\n",
    "\n",
    "                    # output the data\n",
    "                    output_netcdf_obj[name][:] = data\n",
    "                    output_netcdf_obj[name + '_quality_control'][:] = flag\n",
    "        #generate all the attributes for the variables & the global attributes too\n",
    "        generate_netcdf_att(output_netcdf_obj, conf_file_generic, conf_file_point_of_truth=True)\n",
    "        \n",
    "        #now we can output the date/time value (need to have the generation of attributes done first)\n",
    "        time_val_dateobj = date2num(time, output_netcdf_obj['TIME'].units, output_netcdf_obj['TIME'].calendar)\n",
    "        output_netcdf_obj['TIME'][:] = time_val_dateobj\n",
    "\n",
    "        #global attributes from header\n",
    "        for value in globdict:\n",
    "            if dfhead[names[0]].str.contains(value).any():\n",
    "                var = dfhead.loc[dfhead[names[0]].str.contains(value), names[1]].item()\n",
    "                setattr(output_netcdf_obj, globdict[value], var) \n",
    "        # do the date/time stamps individually\n",
    "        datt = dt.datetime.strptime(dfhead.loc[dfhead[names[0]].str.contains('FILE CREATED'),names[1]].item(),'%d-%m-%Y')\n",
    "        setattr(output_netcdf_obj, 'date_created', datt.strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "        setattr(output_netcdf_obj, 'date_modified', dt.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
