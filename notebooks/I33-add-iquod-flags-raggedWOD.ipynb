{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e505f5-6b59-4252-ab17-90e070590ee5",
   "metadata": {},
   "source": [
    "# ðŸ““ I33-add-iquod-flags-raggedWOD\n",
    "\n",
    "**Author:** Thomas Moore ( I'm just trying to run Bec Cowley's work )  \n",
    "**Date:** 2025-04-14  \n",
    "**Updated:** YYYY-MM-DD (if applicable)  \n",
    "**Environment:** `/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans`  \n",
    "**Tags:** troubleshoot, dask\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ Description\n",
    "\n",
    "This notebook my attempt to help trouble shoot https://github.com/CARSv2/cars-v2/blob/iquodFlags/src/features/add_iquod_flags_to_raggedWOD.py .  Issue: [https://github.com/CARSv2/cars-v2/issues/33](https://github.com/CARSv2/cars-v2/issues/33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124345f4-c7b7-4c0e-88af-84c303c6fac2",
   "metadata": {},
   "source": [
    "Author = {\"name\": \"Thomas Moore\", \"affiliation\": \"CSIRO\", \"email\": \"thomas.moore@csiro.au\", \"orcid\": \"0000-0003-3930-1946\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0900f8a-1734-43a0-a881-6abc28e47d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up each WOD ragged array netCDF file and add flags for iQuOD\n",
    "# \"\"\"\n",
    "# The flags are contained in csv files with the following columns:\n",
    "# - WOD unique cast identifier\n",
    "# - iQuOD flag\n",
    "# \"\"\"\n",
    "import logging\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pyarrow.dataset as pa_ds\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15563016-c5de-4372-a4af-b4be6d3d0934",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad060e7-f01e-4560-a7ce-0b1889e748cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_flags_to_wod(flag_file, file_name, out_file):\n",
    "    # open the netcdf file for this dataset\n",
    "    ds = xr.open_dataset(file_name)\n",
    "    # get the unique cast identifiers\n",
    "    wod_unique_cast = ds['wod_unique_cast'].values\n",
    "    # create a new array to hold the flags, that is the same size as the 'Temperature_WODflag' variable\n",
    "    flags = ds['Temperature_WODflag'].copy()\n",
    "    # create a new dataframe to hold the flags\n",
    "    flags_df = pd.DataFrame(flags.values)\n",
    "    # add a column to the dataframe for the cast identifier\n",
    "    flags_df['wod_unique_cast'] = flags_df.index.get_level_values(0)\n",
    "    # set these values to zero for now\n",
    "    flags_df['wod_unique_cast'] = 0\n",
    "    flags_df['depthNumber'] = 0\n",
    "    flags_df['wod_unique_cast'] = flags_df['wod_unique_cast'].astype('int64')\n",
    "    flags_df['depthNumber'] = flags_df['depthNumber'].astype('int64')\n",
    "    # drop the old index\n",
    "    flags_df = flags_df.reset_index(drop=True)\n",
    "    # drop the first column\n",
    "    flags_df = flags_df.drop(columns=0)\n",
    "    # loop over the unique cast identifiers and add the cast identifier to the flags_df for the same number of z_row_size \n",
    "    start = 0\n",
    "    for i, cast in enumerate(wod_unique_cast):\n",
    "        # get the length of the cast\n",
    "        length = ds['Temperature_row_size'][i].values\n",
    "        # fill the wod_unique_cast column with the cast identifier from the start to start + length\n",
    "        flags_df['wod_unique_cast'].values[start:start + int(length)] = cast\n",
    "        # fill the depthNumber column from 0 to length for this cast\n",
    "        flags_df['depthNumber'].values[start:start + int(length)] = range(0, int(length))\n",
    "        # update the start index for the next cast\n",
    "        start = start + int(length)\n",
    "    # read the parquet file with pyarrow and filter it to only include the cast identifiers in the flags_df\n",
    "    dataset = pa_ds.dataset(flag_file, format=\"parquet\")\n",
    "    df_filtered = dataset.to_table(filter=pa_ds.field('wod_unique_cast').isin(wod_unique_cast))\n",
    "    # convert the filtered table to a pandas dataframe\n",
    "    df_filtered = df_filtered.to_pandas()\n",
    "    # merge the flags dataframe with the parquet dataframe on the cast identifier and depth number\n",
    "    flags_df = flags_df.merge(df_filtered, on=['wod_unique_cast', 'depthNumber'], how='left')\n",
    "\n",
    "    # Replace NaN values in the 'Temperature_IQuODflag' column with 0\n",
    "    flags_df['Temperature_IQuODflag'] = flags_df['Temperature_IQuODflag'].fillna(0)\n",
    "    # convert the 'Temperature_IQuODflag' column to int8\n",
    "    flags_df['Temperature_IQuODflag'] = flags_df['Temperature_IQuODflag'].astype('int8')\n",
    "    # remove the 'wod_unique_cast' and 'depthNumber' columns\n",
    "    flags_df = flags_df.drop(columns=['wod_unique_cast', 'depthNumber'])\n",
    "    # remove the index from the flags_df\n",
    "    flags_df = flags_df.reset_index(drop=True)\n",
    "    # convert the flags_df to a pandas dataframe\n",
    "    # flags_df = flags_df.compute()\n",
    "    # convert the flags_df['Temperature_IQuODflag'] column to an xarray data array with all the same dimensions as the original flags variable\n",
    "    flags_da = xr.DataArray(flags_df['Temperature_IQuODflag'].values, dims=flags.dims, coords=flags.coords)\n",
    "    # add the flags_da to the dataset as a new variable\n",
    "    ds['Temperature_IQuODflag'] = flags_da\n",
    "    # update the new variable attributes\n",
    "    ds['Temperature_IQuODflag'].attrs['long_name'] = 'IQuOD quality flag for temperature'\n",
    "    ds['Temperature_IQuODflag'].attrs['flag_values'] = '1, 2, 3, 4'\n",
    "    ds['Temperature_IQuODflag'].attrs['flag_meanings'] = 'passed_all_tests High_True_Postive_Rate_test_failed Compromise_test_failed Low_False_Positive_test_failed'\n",
    "    # update the global attributes for summary, id, creator_name, creator_email, project, date_created, date_modified, publisher_name, publisher_email, publisher_url, history\n",
    "    ds.attrs['summary'] = 'Data for multiple casts from the World Ocean Database with IQuOD quality flags for temperature'\n",
    "    ds.attrs['id'] = file_name + ',' + ds.attrs['id']\n",
    "    ds.attrs['creator_name'] = ds.attrs['creator_name'] + ', ' + 'CSIRO Environment/Ocean Dynamics'\n",
    "    ds.attrs['creator_email'] = ds.attrs['creator_email'] + ', ' + 'https://www.csiro.au/en/contact'\n",
    "    ds.attrs['creator_url'] = ds.attrs['creator_url'] + ', ' + 'https://www.csiro.au'\n",
    "    ds.attrs['project'] = ds.attrs['project'] + ', ' + 'IQuOD (International Quality-controlled Ocean Database)'\n",
    "    ds.attrs['date_created'] = ds.attrs['date_created'] + ', ' + pd.Timestamp.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    ds.attrs['date_modified'] = pd.Timestamp.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    ds.attrs['publisher_name'] = ds.attrs['publisher_name'] + '; ' + 'CSIRO Environment/Ocean Dynamics'\n",
    "    ds.attrs['publisher_email'] = ds.attrs['publisher_email'] + ', ' + 'https://www.csiro.au/en/contact'\n",
    "    ds.attrs['publisher_url'] = ds.attrs['publisher_url'] + ', ' + 'https://www.csiro.au'\n",
    "    ds.attrs['history'] = 'WOD downloaded on January 3, 2025 with IQuOD quality flags added to temperature variable'\n",
    "    # put a cc4.0 license on the dataset\n",
    "    ds.attrs['license'] = 'https://creativecommons.org/licenses/by/4.0/legalcode'\n",
    "    # save the modified dataset\n",
    "    ds.to_netcdf(out_file)\n",
    "    logger.info(f\"Saved new file with flags: {out_file}\")\n",
    "\n",
    "\n",
    "def convert_csv2parquet(csv_file, parquet_file):\n",
    "    # start a dask client\n",
    "    client = Client()\n",
    "    # open the csv file for this dataset as a dataframe using\n",
    "    df = dd.read_csv(csv_file, names=['wod_unique_cast', 'depthNumber','Temperature_IQuODflag'], header=None, dtype={'wod_unique_cast': 'int64', 'depthNumber': 'int64', 'Temperature_IQuODflag': 'int8'})\n",
    "    # write the updated parquet file\n",
    "    try:\n",
    "        print(f'Saving Parquet file: {parquet_file}')\n",
    "        df.to_parquet(parquet_file, compression='snappy')\n",
    "        print(f'Successfully saved Parquet file: {parquet_file}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Parquet file {parquet_file}: {e}\")\n",
    "    # close the dask client\n",
    "    client.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a261c8-0228-4128-9ce6-44f4df2cb767",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4674f1ab-8dfd-44b5-9611-6b4b83272627",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fmt = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "logger = logging.getLogger(__name__)\n",
    "# set up the input and output file paths\n",
    "folder = '/scratch/es60/thomas_moore/AQC_summaries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c7ebc7-c60b-4601-9551-5ca397111208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the datasets from the folder where datasets are the first three characters of the file names\n",
    "# datasets = sorted(set([f[:3] for f in os.listdir(folder) if f.endswith('.csv')]))\n",
    "# remove the XBT dataset from the list as we have already processed it\n",
    "# datasets.remove('XBT')\n",
    "datasets = ['XBT']\n",
    "WOD_path = '/scratch/es60/rlc599/WOD'\n",
    "out_path = '/scratch/es60/thomas_moore/IQuOD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4847d82-ecad-4c65-a5dd-f1392e07cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List only directories in WOD_path\n",
    "years = sorted([d for d in os.listdir(WOD_path) if os.path.isdir(os.path.join(WOD_path, d))])\n",
    "# remove years prior to 1992\n",
    "years = [year for year in years if int(year) >= 1992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87340f0f-8476-4b60-842c-c0aea3ae28e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '2020',\n",
       " '2021',\n",
       " '2022',\n",
       " '2023',\n",
       " '2024']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fbd57-cede-452c-a6dc-42555472937c",
   "metadata": {},
   "source": [
    "# limit to one year for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87a2d82-d5a2-45e6-94ad-fdde4c6fa3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['1992']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e50c85e-daa8-4383-9b1a-bd24f2b19356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 16:02:18,990 - __main__ - INFO - Creating parquet file for XBT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Parquet file: /scratch/es60/thomas_moore/AQC_summaries/xbt_flags.parquet\n",
      "Successfully saved Parquet file: /scratch/es60/thomas_moore/AQC_summaries/xbt_flags.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 16:02:51,123 - __main__ - INFO - Writing flags to file: /scratch/es60/rlc599/WOD/1992/wod_xbt_1992.nc\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:24\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mwrite_flags_to_wod\u001b[0;34m(flag_file, file_name, out_file)\u001b[0m\n\u001b[1;32m     72\u001b[0m ds\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicense\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://creativecommons.org/licenses/by/4.0/legalcode\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# save the modified dataset\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved new file with flags: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/core/dataset.py:2329\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   2326\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 2329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/backends/api.py:1360\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;66;03m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;66;03m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;66;03m# to be parallelized with dask\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m     \u001b[43mdump_to_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoclose:\n\u001b[1;32m   1364\u001b[0m         store\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/backends/api.py:1407\u001b[0m, in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder:\n\u001b[1;32m   1405\u001b[0m     variables, attrs \u001b[38;5;241m=\u001b[39m encoder(variables, attrs)\n\u001b[0;32m-> 1407\u001b[0m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/backends/common.py:363\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.store\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ArrayWriter()\n\u001b[0;32m--> 363\u001b[0m variables, attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_dimensions(variables, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims)\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/backends/common.py:452\u001b[0m, in \u001b[0;36mWritableCFDataStore.encode\u001b[0;34m(self, variables, attributes)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, variables, attributes):\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# All NetCDF files get CF encoded by default, without this attempting\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;66;03m# to write times, for example, would fail.\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m     variables, attributes \u001b[38;5;241m=\u001b[39m \u001b[43mcf_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     variables \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_variable(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    454\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_attribute(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m attributes\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/conventions.py:805\u001b[0m, in \u001b[0;36mcf_encoder\u001b[0;34m(variables, attributes)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# add encoding for time bounds variables if present.\u001b[39;00m\n\u001b[1;32m    803\u001b[0m _update_bounds_encoding(variables)\n\u001b[0;32m--> 805\u001b[0m new_vars \u001b[38;5;241m=\u001b[39m {k: encode_cf_variable(v, name\u001b[38;5;241m=\u001b[39mk) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# Remove attrs from bounds variables (issue #2921)\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m new_vars\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/conventions.py:805\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# add encoding for time bounds variables if present.\u001b[39;00m\n\u001b[1;32m    803\u001b[0m _update_bounds_encoding(variables)\n\u001b[0;32m--> 805\u001b[0m new_vars \u001b[38;5;241m=\u001b[39m {k: \u001b[43mencode_cf_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# Remove attrs from bounds variables (issue #2921)\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m new_vars\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/conventions.py:196\u001b[0m, in \u001b[0;36mencode_cf_variable\u001b[0;34m(var, needs_copy, name)\u001b[0m\n\u001b[1;32m    183\u001b[0m ensure_not_multiindex(var, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m coder \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    186\u001b[0m     times\u001b[38;5;241m.\u001b[39mCFDatetimeCoder(),\n\u001b[1;32m    187\u001b[0m     times\u001b[38;5;241m.\u001b[39mCFTimedeltaCoder(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     variables\u001b[38;5;241m.\u001b[39mBooleanCoder(),\n\u001b[1;32m    195\u001b[0m ]:\n\u001b[0;32m--> 196\u001b[0m     var \u001b[38;5;241m=\u001b[39m \u001b[43mcoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# TODO(kmuehlbauer): check if ensure_dtype_not_object can be moved to backends:\u001b[39;00m\n\u001b[1;32m    199\u001b[0m var \u001b[38;5;241m=\u001b[39m ensure_dtype_not_object(var, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/coding/times.py:969\u001b[0m, in \u001b[0;36mCFDatetimeCoder.encode\u001b[0;34m(self, variable, name)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, variable: Variable, name: T_Name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Variable:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\n\u001b[0;32m--> 969\u001b[0m         \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdatetime64\n\u001b[1;32m    970\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m contains_cftime_datetimes(variable):\n\u001b[1;32m    971\u001b[0m         dims, data, attrs, encoding \u001b[38;5;241m=\u001b[39m unpack_for_encoding(variable)\n\u001b[1;32m    973\u001b[0m         units \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/core/variable.py:449\u001b[0m, in \u001b[0;36mVariable.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, indexing\u001b[38;5;241m.\u001b[39mExplicitlyIndexed):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/core/indexing.py:837\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 837\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/core/indexing.py:831\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/core/indexing.py:788\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/core/indexing.py:658\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n\u001b[0;32m--> 658\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_numpy_scalars(array)\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/coding/variables.py:81\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.get_duck_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/coding/times.py:374\u001b[0m, in \u001b[0;36mdecode_cf_timedelta\u001b[0;34m(num_timedeltas, units)\u001b[0m\n\u001b[1;32m    372\u001b[0m num_timedeltas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(num_timedeltas)\n\u001b[1;32m    373\u001b[0m units \u001b[38;5;241m=\u001b[39m _netcdf_to_numpy_timeunit(units)\n\u001b[0;32m--> 374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mto_timedelta_unboxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_timedeltas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reshape(result, num_timedeltas\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/xarray/coding/times.py:357\u001b[0m, in \u001b[0;36mto_timedelta_unboxed\u001b[0;34m(value, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_timedelta_unboxed\u001b[39m(value, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 357\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_timedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimedelta64[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/pandas/core/tools/timedeltas.py:213\u001b[0m, in \u001b[0;36mto_timedelta\u001b[0;34m(arg, unit, errors)\u001b[0m\n\u001b[1;32m    211\u001b[0m     arg \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mitem_from_zerodim(arg)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg must be a string, timedelta, list, tuple, 1-d array, or Series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     )\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/pandas/core/tools/timedeltas.py:266\u001b[0m, in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, unit, errors, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     td64arr \u001b[38;5;241m=\u001b[39m \u001b[43msequence_to_td64ns\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/g/data/es60/users/thomas_moore/miniconda3/envs/tabular_oceans/lib/python3.10/site-packages/pandas/core/arrays/timedeltas.py:1059\u001b[0m, in \u001b[0;36msequence_to_td64ns\u001b[0;34m(data, copy, unit, errors)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1057\u001b[0m     mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(data)\n\u001b[0;32m-> 1059\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mcast_from_unit_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m data[mask] \u001b[38;5;241m=\u001b[39m iNaT\n\u001b[1;32m   1061\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm8[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mconversion.pyx:145\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.cast_from_unit_vectorized\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for dataset in datasets:\n",
    "    # if parquet files are not available, then create them\n",
    "    flag_file = os.path.join(folder, dataset.lower() + '_flags.parquet')\n",
    "    if not os.path.exists(flag_file):\n",
    "        logger.info(f\"Creating parquet file for {dataset}\")\n",
    "        # read the csv file for this dataset\n",
    "        csv_file = os.path.join(folder, dataset + '_summary.csv')\n",
    "        convert_csv2parquet(csv_file, flag_file)\n",
    "    # loop through the years\n",
    "    for year in years:\n",
    "        # Open the netcdf file for this dataset\n",
    "        file_name = os.path.join(WOD_path, year, 'wod_' + dataset.lower() + '_' + year + '.nc')\n",
    "        if not os.path.exists(file_name):\n",
    "            logger.info(f\"File not found: {file_name}\")\n",
    "            continue\n",
    "        # create the output file name\n",
    "        out_file = os.path.join(out_path, year, 'iquod_' + dataset.lower() + '_' + year + '_iquodflags.nc')\n",
    "        # check if the output path exists, if not create it\n",
    "        out_dir = os.path.dirname(out_file)\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        # write the flags to the parquet files\n",
    "        logger.info(f\"Writing flags to file: {file_name}\")\n",
    "        write_flags_to_wod(flag_file, file_name, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b58708-6b06-4e5c-a19f-4758cc024ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a93f8-7bb6-4f35-a34d-cbc3f04e00ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3251c2-8695-44b0-ac21-7e6fb0b87bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace8e5e-ae28-4036-8072-42096f37664f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfad252-60c4-4716-a8b9-5163d23eed0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62833fe1-423c-4678-ab51-6826a370dd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8682f3-1f5f-4d92-8bad-8f0d5109ac0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc1d50-7168-4c5e-b259-c95dd6178f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68beab-61ea-4b5c-8d82-0fb85feec23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff334120-e664-48a5-9452-3ddc9181aebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcf666-0564-4e46-b44b-567fdbe5a1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c82187-5a74-467f-818a-e5bb734df38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80895a-0638-477f-8eba-2a3ddfda5eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b99bd-b538-4716-bb08-ec24a47271e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c3414-8758-47eb-ae64-1993fbbf70ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca44e4-f53d-459a-9063-e26e1e98bbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a34fa-1143-4d34-b079-36e9c3c8f10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661595e-2bf6-4720-8564-e4e247c9aa32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f381795d-f032-44e2-a836-111d755a7550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2270f05-c235-4cd6-a2de-16a8d3959f40",
   "metadata": {},
   "source": [
    "#### Date: 11 September 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b0004-214b-4015-bf93-e9e2b2f98c65",
   "metadata": {},
   "source": [
    "# detect compute platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fdfb1c-d68b-4693-938a-028037db64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "    \n",
    "def get_platform():\n",
    "    hostname = socket.gethostname()\n",
    "    if \"gadi\" in hostname:  # Adjust this condition to fit your HPC's hostname or unique identifier\n",
    "        return 'HPC',hostname\n",
    "    else:\n",
    "        return 'Laptop',hostname\n",
    "[platform,hostname] = get_platform()\n",
    "print('the platform we are working on is '+platform+' with hostname: '+hostname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3276b1-c186-4b4a-81f6-2b3328ae6004",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c387ac-757c-488b-b34b-50c6b2389578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import polars as pl\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248ad4e-3a28-4610-b4ef-56191d13da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42083159-1001-4d97-9243-0caade722bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == 'HPC':\n",
    "    data_path = '/g/data/es60/users/thomas_moore/CODA/2022/'\n",
    "    write_path = '/g/data/es60/users/thomas_moore/CODA/parquet/'\n",
    "else:\n",
    "    data_path = '/Users/moo270/data/CARSv2/CODA/'\n",
    "    write_path = '/Users/moo270/data/CARSv2/CODA/parquet/'\n",
    "\n",
    "print(f\"Using data path: {data_path}\")\n",
    "print(f\"Using write path: {write_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622648ff-84c7-4d52-b223-144486a91aef",
   "metadata": {},
   "source": [
    "# build PQ from NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee8e81-0b29-4949-9849-855f253de129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = xr.open_dataset(data_path+ \"WOD_CODA_2022_pfl.nc\",chunks=\"auto\")\n",
    "ds = xr.open_dataset(data_path+ \"WOD_CODA_2022_pfl.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d9673-5b4c-4763-b1e8-315e2f7a9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b275b-38a5-4ea8-956f-c95945ef690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.set_coords(['WOD_id',\n",
    "                    'origflagset',\n",
    "                    'country',\n",
    "                    'dataset',\n",
    "                    'Access_no',\n",
    "                    'dbase_orig',\n",
    "                    'Project',\n",
    "                    'WOD_cruise_identifier',\n",
    "                    'Institute',\n",
    "                    'Ocean_Vehicle',\n",
    "                    'Temperature_Instrument',\n",
    "                    'CODA_id',\n",
    "                    'Recorder',\n",
    "                    'Platform',\n",
    "                    'crs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93835245-eeea-4b0c-acc6-b29dec28b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.set_coords(['WOD_id',\n",
    "                    'origflagset',\n",
    "                    'country',\n",
    "                    'dataset',\n",
    "                    'Access_no',\n",
    "                    'dbase_orig',\n",
    "                    'Project',\n",
    "                    'WOD_cruise_identifier',\n",
    "                    'Institute',\n",
    "                    'Ocean_Vehicle',\n",
    "                    'Temperature_Instrument',\n",
    "                    'CODA_id',\n",
    "                    'crs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7877e-62bf-4a99-88bd-b34f864d0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d569e-ef09-4592-bb1e-27fb014592b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.nbytes/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be97945-cd32-4b18-8a05-09143517a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = ds[['Temperature',\n",
    "                   'Temperature_WODflag',\n",
    "                   'Temperature_origflag',\n",
    "                   'Salinity',\n",
    "                   'Salinity_WODflag',\n",
    "                   'Salinity_origflag',\n",
    "                   'Oxygen',\n",
    "                   'Oxygen_WODflag',\n",
    "                   'Oxygen_origflag',\n",
    "                   'Chlorophyll',\n",
    "                   'Chlorophyll_WODflag',\n",
    "                   'Chlorophyll_origflag',\n",
    "                   'Nitrate',\n",
    "                   'Nitrate_WODflag',\n",
    "                   'Nitrate_origflag',\n",
    "                   'pH',\n",
    "                   'pH_WODflag',\n",
    "                   'pH_origflag'\n",
    "                  ]].to_dataframe().reset_index()\n",
    "df\n",
    "print('done loading DF from 104GB object')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecf923e5-77bd-4405-bc27-fb07879f3c61",
   "metadata": {},
   "source": [
    "%%time\n",
    "df = ds[['Temperature',\n",
    "                   'Temperature_WODflag',\n",
    "                   'Temperature_origflag',\n",
    "                   'Salinity',\n",
    "                   'Salinity_WODflag',\n",
    "                   'Salinity_origflag',\n",
    "                   'Oxygen',\n",
    "                   'Oxygen_WODflag',\n",
    "                   'Oxygen_origflag',\n",
    "                   'Chlorophyll',\n",
    "                   'Chlorophyll_WODflag',\n",
    "                   'Chlorophyll_origflag'\n",
    "                  ]].to_dataframe().reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd47506-b782-413d-82bd-c77a15ae09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the memory usage in bytes\n",
    "size_in_bytes = df.memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert to gigabytes (GB)\n",
    "size_in_gb = size_in_bytes / 1e9\n",
    "\n",
    "print(f\"Size of the DataFrame: {size_in_gb} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f265688-acb1-417b-b24d-8795d2a8a636",
   "metadata": {},
   "source": [
    "# Try removing all-column-NaN-rows here? - https://github.com/CARSv2/cars-v2/issues/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45d13e-cd47-4d8d-9a62-38051b02a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_squeeze = df.dropna(subset=['Temperature',\n",
    "                   'Temperature_WODflag',\n",
    "                   'Temperature_origflag',\n",
    "                   'Salinity',\n",
    "                   'Salinity_WODflag',\n",
    "                   'Salinity_origflag',\n",
    "                   'Oxygen',\n",
    "                   'Oxygen_WODflag',\n",
    "                   'Oxygen_origflag',\n",
    "                   'Chlorophyll',\n",
    "                   'Chlorophyll_WODflag',\n",
    "                   'Chlorophyll_origflag'],how='all').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80bc94-1fbb-4488-8d3d-e3a717d8ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbecf6-928e-491e-94c6-0eb45d275564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the memory usage in bytes\n",
    "size_in_bytes = df_squeeze.memory_usage(deep=True).sum()\n",
    "\n",
    "# Convert to gigabytes (GB)\n",
    "size_in_gb = size_in_bytes / 1e9\n",
    "\n",
    "print(f\"Size of the DataFrame: {size_in_gb} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977066e-8845-41fe-8e39-9710f475aa7f",
   "metadata": {},
   "source": [
    "# write this out to PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c24be-deab-4910-8ec2-7335356a0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Writing the Pandas DataFrame to a Parquet file\n",
    "df_squeeze.to_parquet(write_path+'2022_pfl_df_squeeze.pq', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9f1dd-54ac-4e75-ace8-06ff4c40f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done writing PQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc51a00-9fa8-4080-9970-1838d9d64c76",
   "metadata": {},
   "source": [
    "# load directly with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a47f1-a051-4d12-b71b-cedc82c7edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(write_path+'2005_pfl_df.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797e8d6-d095-4629-b27b-ac4692d669cf",
   "metadata": {},
   "source": [
    "##### find columns with byte strings and convert ( is this needed when going from ds --> df --> ddf )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9432f8db-4bba-465f-a142-bff4d71424eb",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Function to convert byte strings to regular strings\n",
    "def remove_binary_prefix(val):\n",
    "    if isinstance(val, bytes):  # check if the value is bytes\n",
    "        return val.decode('utf-8')  # decode bytes to string\n",
    "    return val\n",
    "\n",
    "# Convert byte string columns to regular string columns in ddf\n",
    "ddf_byte_cleaned = ddf.map_partitions(lambda df: df.map(remove_binary_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e076f-facc-4df3-a3d9-9afbe4e3d597",
   "metadata": {},
   "source": [
    "# Polars: for single node, smaller-than-memory operations that are faster than `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ad30d-1f4c-44f2-bc2c-6e5e8c3205d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_df = pl.read_parquet(write_path+\"2005_pfl_df_squeeze.pq\")\n",
    "pdf = polars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ab4cd-24c1-425e-bb13-959717e11045",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a2c8b-45b2-421c-9b0f-a72ad734fd4c",
   "metadata": {},
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f42a1-b38c-4f95-b74e-b89d48606521",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Search for rows where 'column_name' matches a condition\n",
    "filtered_pdf = pdf.filter(pl.col('z') >= 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bcab9-afdb-4e38-8597-06e778748584",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5bf6e-b98a-4173-9d11-60cd2c430c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Search for rows where 'column_name' matches a condition\n",
    "#filtered_pdf = pdf.filter((pl.col('z') >= 50 )& (pl.col('z') <= 150))\n",
    "filtered_pdf = pdf.filter((pl.col('z') >= 1750 ))\n",
    "\n",
    "# Step 2: Convert Polars DataFrame to Pandas for compatibility with CartoPy\n",
    "df = filtered_pdf.to_pandas()\n",
    "\n",
    "# Step 3: Plotting with CartoPy\n",
    "# Create a map projection (e.g., PlateCarree for lat/lon)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add features like coastlines, etc.\n",
    "ax.coastlines()\n",
    "\n",
    "# Step 4: Plot the lat/lon points with their corresponding values\n",
    "sc = ax.scatter(df['lon'], df['lat'], c=df['Temperature'], cmap='viridis', s=1, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add a colorbar to indicate the values\n",
    "plt.colorbar(sc, label='Temperature')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c52f0-5f7a-4246-8ef3-8bb04a8dc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Step 1: Filter the Polars DataFrame (as in your original code)\n",
    "# filtered_pdf = pdf.filter((pl.col('z') >= 50 )& (pl.col('z') <= 150))\n",
    "filtered_pdf = pdf.filter((pl.col('Temperature') >= 65))\n",
    "\n",
    "# Step 2: Convert Polars DataFrame to Pandas for compatibility with CartoPy\n",
    "df = filtered_pdf.to_pandas()\n",
    "\n",
    "# Step 3: Plotting with CartoPy\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Add features like coastlines\n",
    "ax.coastlines()\n",
    "# Add the land feature and specify the color\n",
    "land_feature = cfeature.LAND.with_scale('110m')  # Use '110m' for a low-resolution map (faster plotting)\n",
    "ax.add_feature(land_feature, facecolor='tan')\n",
    "\n",
    "# Optionally, add ocean with a different color\n",
    "ocean_feature = cfeature.OCEAN.with_scale('110m')\n",
    "ax.add_feature(ocean_feature, facecolor='lightblue')\n",
    "\n",
    "# Step 4: Plot the lat/lon points with their corresponding values\n",
    "sc = ax.scatter(df['lon'], df['lat'], c=df['Temperature'], cmap='viridis', s=1, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add a colorbar to indicate the values\n",
    "plt.colorbar(sc, label='Temperature')\n",
    "\n",
    "# Step 5: Annotate the points with their z values\n",
    "for i, row in df.iterrows():\n",
    "    ax.annotate(f\"{row['z']:.1f} m\", (row['lon'], row['lat']),\n",
    "                xytext=(3, 3),  # Offset for the text to not overlap with the point\n",
    "                textcoords='offset points',  # Position the text relative to the point\n",
    "                fontsize=6,  # Adjust the fontsize\n",
    "                color='black')  # Color of the text\n",
    "\n",
    "# Display the plot\n",
    "ax.set_extent([-70, -50, 0, 20], crs=ccrs.PlateCarree())  # Full world map extent\n",
    "plt.title('Ocean Temps over 65 degrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2bf30-d858-48b8-a866-bb9a0662607f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394592f8-f113-42e7-a5ea-28a189d819d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128ae6a-f400-4426-ad7f-188978254980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897f7cf-c77b-4897-bdd4-0ee6499b3fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93a00b-a2e5-489b-b218-0baf216ac8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd5bda-3adc-447b-afbe-ff30c80f8fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98fa70a3-0e1a-4ac3-a9a7-2ca83886b85c",
   "metadata": {},
   "source": [
    "# list of todo's\n",
    "\n",
    "- create parquet from a df\n",
    "- consider utility of turning \"metadata\" vars into coords\n",
    "- consider removing NaN's at df step\n",
    "- consider removing NaN's otherwise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe423ce9-e5ab-40fd-bd64-8dc48e555569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='all',column='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd6561-f895-4627-a91b-b24a5a71e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = ds.to_dataframe()#.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24a869-bb67-4b60-b118-49ab4dd0dd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882aa6f-e071-49bc-ae78-ceb66181476d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
